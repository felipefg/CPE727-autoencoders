\documentclass{beamer}
\usepackage{amsfonts,amsmath,oldgerm}
\usepackage{tikz}
\usepackage[portuguese]{babel}
\usetikzlibrary{arrows.meta,calc}
\usetheme{sintef}

\newcommand{\testcolor}[1]{\colorbox{#1}{\textcolor{#1}{test}}~\texttt{#1}}

\usefonttheme[onlymath]{serif}

\titlebackground*{assets/background}

% adicionar o numero na lista final da apresentação
\setbeamertemplate{bibliography item}{\insertbiblabel}

\newcommand{\hrefcol}[2]{\textcolor{cyan}{\href{#1}{#2}}}

\title{Autoencoders: da motivação às variantes modernas}
\subtitle{}
\course{CPE 727 - Aprendizado de Profundo}
\author{Felipe Fink Grael, Rafael Tadeu Cardoso dos Santos, Thalles Nonato Leal Santos e Jefferson Osowsky}
\begin{document}
\maketitle

% ============================
% Autoencoder: Motivação
% ============================

\section{Motivação}

\begin{frame}{Introdução}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \begin{itemize}
                \item \textbf{Definição:} Algoritmos cujo propósito principal é copiar sua entrada na saída \cite{Rumelhart1986}. São tipicamente construídos como redes neurais artificiais treinadas de forma não supervisionada.
                \item \textbf{Arrquitetura Básica:}
                \begin{itemize}
                    \item Encoder: transforma entrada em representação latente
                    \item Representação: espaço latente de menor, maior ou igual dimensão
                    \item Decoder: reconstrói a entrada original
                \end{itemize}
                \item \textbf{Objetivo:} Aprender a função identidade \(f(x) \approx x\) através de um espaço latente comprimido
            \end{itemize}
        \end{column}
        \begin{column}{0.25\textwidth}
            \begin{figure}[h]
                \centering
                \includegraphics[width=\textwidth]{assets/autoencoders.png}
                \caption{Estrutura básica de Autoencoders.}
                \label{fig:autoencoders}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Componentes Fundamentais}
    \framesubtitle{}

    \begin{figure}[h]
                \centering
                \includegraphics[width=0.55\textwidth]{assets/autoencoder_mnist_example.png}
                \caption{Estrutura de um autoencoder com representação Latente\footnote{Figura de Umberto Michelucci \cite{UmbertoMichelucci2022}}.}
                \label{fig:autoencoders}
            \end{figure}
    \vspace{-2mm}
    \begin{itemize}
        \item \textbf{Encoder:} $f_\theta: \mathcal{X} \rightarrow \mathcal{H}$ onde $h = f_\theta(x)$
        \item \textbf{Decoder:} $g_\phi: \mathcal{H} \rightarrow \mathcal{X}$ onde $x' = g_\phi(h)$
        \item \textbf{Reconstrução:} $x' = g_\phi(f_\theta(x))$
        \item \textbf{Espaço latente $\mathcal{H}$:} Representação comprimida dos dados ($\dim(\mathcal{H}) < \dim(\mathcal{X})$)
    \end{itemize}
    \vspace{2mm}
\end{frame}

\begin{frame}{Por que usar Autoencoders?}
    \textbf{Aprendizado de Representações:}

    \begin{itemize}
        \item Extrair características relevantes automaticamente dos dados
        \item Redução de dimensionalidade não-linear (superior ao PCA para dados complexos)
        \item Aprendizado não supervisionado - não requer labels
    \end{itemize}

    \textbf{Vantagens sobre métodos tradicionais:}
    \begin{itemize}
        \item PCA: apenas transformações lineares
        \item Autoencoders: capturam relações não-lineares complexas
        \item Profundidade permite representações hierárquicas \cite{Tschannen2018}
    \end{itemize}

\end{frame}

\section{Formulação Matemática}
\begin{frame}{Definição Formal}
    Seja $\mu_{ref}$ uma distribuição de probabilidade de referência em $\mathcal{X}$ e $d: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ uma função de distância.

    \vspace{2mm}
    Função de custo do autoencoder:
    $$
    L(\theta, \phi) = \mathbb{E}_{x \sim \mu_{ref}} \left[ d\left( x, g_\phi(f_\theta(x)) \right) \right]
    $$

    Objetivo de treinamento:
    $$
    (\theta^*, \phi^*) = \arg\min_{\theta, \phi} L(\theta, \phi)
    $$

\end{frame}

\begin{frame}{Exemplo: Autoencoder Linear de Uma Camada}
    \textbf{Encoder:}
    $$
    h = f_{W,b}(x) = \sigma(Wx + b)
    $$
    \vspace{1mm}
    \textbf{Decoder:}
    $$
    x' = g_{W',b'}(h) = \sigma(W'h + b')
    $$


    \vspace{1mm}
    \textbf{Função Custo:}
    $$
    L(W, b, W', b') = \frac{1}{N} \sum_{i=1}^{N} \| x_i - g_{W',b'}(f_{W,b}(x_i)) \|_2^2
    $$

    Parâmetros a otimizar: $\theta = {W, b}$, $\phi = {W', b'}$

\end{frame}

\begin{frame}{Exemplo: Undercomplete Autoencoder Linear (Caso Especial)}
    \textbf{Autoencoder linear:} sem função de ativação $\sigma(z) = z$

    \vspace{2mm}

    \textbf{Teorema}: O autoencoder linear ótimo projeta os dados no subespaço gerado pelos primeiros $k$ autovetores da matriz de covariância $\Sigma_{XX}$ \cite{Oja1982SimplifiedNM}.

    \vspace{2mm}
    Erro mínimo:

    $$
    \Sigma(A, B) = Tr(\Sigma) - \sum_{i=1}^{k} \lambda_i = \sum_{i=k+1}^{n} \lambda_i
    $$

    Onde $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n$ são os autovalores de $\Sigma_{XX}$.

    \textbf{Conexão com PCA}: Autoencoders lineares aprendem o mesmo subespaço que a Análise de Componentes Principais.

\end{frame}

\section{Referências Bibliográficas}
\begin{frame}[allowframebreaks]
        \frametitle{Referências Bibliográficas}
        \bibliographystyle{ieeetr}
        \bibliography{presentation_bib.bib}
\end{frame}

\backmatter
\end{document}
