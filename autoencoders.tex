\documentclass{beamer}
\usepackage{amsfonts,amsmath,oldgerm}
\usepackage{tikz}
\usepackage[portuguese]{babel}
\usetikzlibrary{arrows.meta,calc}
\usetheme{sintef}

\newcommand{\testcolor}[1]{\colorbox{#1}{\textcolor{#1}{test}}~\texttt{#1}}

\usefonttheme[onlymath]{serif}

\titlebackground*{assets/background}

% adicionar o numero na lista final da apresentação
\setbeamertemplate{bibliography item}{\insertbiblabel}

\newcommand{\hrefcol}[2]{\textcolor{cyan}{\href{#1}{#2}}}

\title{Autoencoders: da motivação às variantes modernas}
\subtitle{}
\course{CPE 727 - Aprendizado de Profundo}
\author{Felipe Fink Grael, Rafael Tadeu Cardoso dos Santos, Thalles Nonato Leal Santos e Jefferson Osowsky}
\begin{document}
\maketitle

% ============================
% Autoencoder: Motivação
% ============================

\section{Motivação}

\begin{frame}{Introdução}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \begin{itemize}
                \item \textbf{Definição:} Algoritmos cujo propósito principal é copiar sua entrada na saída \cite{Rumelhart1986}. São tipicamente construídos como redes neurais artificiais treinadas de forma não supervisionada.
                \item \textbf{Arrquitetura Básica:}
                \begin{itemize}
                    \item Encoder: transforma entrada em representação latente
                    \item Representação: espaço latente de menor, maior ou igual dimensão
                    \item Decoder: reconstrói a entrada original
                \end{itemize}
                \item \textbf{Objetivo:} Aprender a função identidade \(f(x) \approx x\) através de um espaço latente comprimido
            \end{itemize}
        \end{column}
        \begin{column}{0.25\textwidth}
            \begin{figure}[h]
                \centering
                \includegraphics[width=\textwidth]{assets/autoencoders.png}
                \caption{Estrutura básica de Autoencoders.}
                \label{fig:autoencoders}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}
\begin{frame}{Componentes Fundamentais}
    \framesubtitle{}

    \begin{figure}[h]
                \centering
                \includegraphics[width=0.55\textwidth]{assets/autoencoder_mnist_example.png}
                \caption{Estrutura de um autoencoder com representação latente\footnote{Figura de Umberto Michelucci \cite{UmbertoMichelucci2022}}.}
                \label{fig:autoencoders}
            \end{figure}
    \vspace{-2mm}
    \begin{itemize}
        \item \textbf{Encoder:} $f_\theta: \mathcal{X} \rightarrow \mathcal{H}$ onde $h = f_\theta(x)$
        \item \textbf{Decoder:} $g_\phi: \mathcal{H} \rightarrow \mathcal{X}$ onde $x' = g_\phi(h)$
        \item \textbf{Reconstrução:} $x' = g_\phi(f_\theta(x))$
        \item \textbf{Espaço latente $\mathcal{H}$:} Representação comprimida dos dados ($\dim(\mathcal{H}) < \dim(\mathcal{X})$)
    \end{itemize}
    \vspace{2mm}
\end{frame}

\begin{frame}{Por que usar Autoencoders?}
    \textbf{Aprendizado de Representações:}

    \begin{itemize}
        \item Extrair características relevantes automaticamente dos dados
        \item Redução de dimensionalidade não-linear (superior ao PCA para dados complexos)
        \item Aprendizado não supervisionado - não requer labels
    \end{itemize}

    \textbf{Vantagens sobre métodos tradicionais:}
    \begin{itemize}
        \item PCA: apenas transformações lineares
        \item Autoencoders: capturam relações não-lineares complexas
        \item Profundidade permite representações hierárquicas \cite{Tschannen2018}
    \end{itemize}

\end{frame}

\section{Formulação Matemática}
\begin{frame}{Definição Formal}
    Seja $\mu_{ref}$ uma distribuição de probabilidade de referência em $\mathcal{X}$ e $d: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ uma função de distância.

    \vspace{2mm}
    Função de custo do autoencoder:
    $$
    L(\theta, \phi) = \mathbb{E}_{x \sim \mu_{ref}} \left[ d\left( x, g_\phi(f_\theta(x)) \right) \right]
    $$

    Objetivo de treinamento:
    $$
    (\theta^*, \phi^*) = \arg\min_{\theta, \phi} L(\theta, \phi)
    $$

\end{frame}

\begin{frame}{Exemplo: Autoencoder Linear de Uma Camada}
    \textbf{Encoder:}
    $$
    h = f_{W,b}(x) = \sigma(Wx + b)
    $$
    \vspace{1mm}
    \textbf{Decoder:}
    $$
    x' = g_{W',b'}(h) = \sigma(W'h + b')
    $$


    \vspace{1mm}
    \textbf{Função Custo:}
    $$
    L(W, b, W', b') = \frac{1}{N} \sum_{i=1}^{N} \| x_i - g_{W',b'}(f_{W,b}(x_i)) \|_2^2
    $$

    Parâmetros a otimizar: $\theta = {W, b}$, $\phi = {W', b'}$

\end{frame}

\begin{frame}{Exemplo: Undercomplete Autoencoder Linear (Caso Especial)}
    \textbf{Autoencoder linear:} sem função de ativação $\sigma(z) = z$

    \vspace{2mm}

    \textbf{Teorema}: O autoencoder linear ótimo projeta os dados no subespaço gerado pelos primeiros $k$ autovetores da matriz de covariância $\Sigma_{XX}$ \cite{Oja1982SimplifiedNM}.

    \vspace{2mm}
    Erro mínimo:

    $$
    \Sigma(A, B) = Tr(\Sigma) - \sum_{i=1}^{k} \lambda_i = \sum_{i=k+1}^{n} \lambda_i
    $$

    Onde $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_n$ são os autovalores de $\Sigma_{XX}$.

    \textbf{Conexão com PCA}: Autoencoders lineares aprendem o mesmo subespaço que a Análise de Componentes Principais.

\end{frame}

\section{Undercomplete Autoencoders com redes neurais}

\begin{frame}{Undercomplete Autoenconder}

\begin{itemize}
    \item Autoencoders são treinados para copiar a entrada para a saída, mas o objetivo real é aprender representações úteis dos dados.

    \item Para isso, impõe-se que a dimensão do \textit{espaço latente} satisfaça 
    $\dim(h) < \dim(x)$, formando um \textit{autoencoder undercomplete}, o que força o modelo a capturar as características mais relevantes da distribuição dos dados.

    \item O treinamento consiste em minimizar a função de perda:
    \[
        L(x, g(f(x))),
    \]
    onde $L$ mede quão diferente $g(f(x))$ está de $x$ (por exemplo, usando MSE).
\end{itemize}
\end{frame}


\begin{frame}{Undercomplete Autoenconders: Correspondência com PCA/SVD}
    \textbf{PCA e Autoencoders:} 
    Quando o decoder é linear e $L$ é o erro quadrático médio (MSE), 
    um autoencoder undercomplete aprende a abranger o mesmo subespaço que o PCA.
    \vspace{3mm}

    Isso ocorre porque, sob linearidade e codificação com dimensão reduzida, 
    o autoencoder busca a melhor reconstrução no sentido dos mínimos quadrados,
    exatamente como o PCA.
\end{frame}

\begin{frame}{Formulação do Problema}

    O objetivo de um autoencoder linear é resolver:

    \[
        \min_{D,E}\ \frac{1}{2}\|X - D(E(X))\|_F^2.
    \]

    \vspace{2mm}
    
    Se escolhemos $D$ e $E$ como matrizes (mapeamentos lineares), o problema é:

    \[
        \min_{D,E}\ \frac{1}{2}\|X - DEX\|_F^2.
    \]

    Introduzimos uma variável auxiliar:

    \begin{itemize}
        \item \(A = DE\)
        \item \(\min_{A,D,E}\ \frac{1}{2}\|X - AX\|_F^2 
               \quad \text{sujeito a } A = DE.\)
        \item   A restrição $A = DE$ é o que torna o problema não convexo.
    \end{itemize}
    \vspace{2mm}
\end{frame}



\begin{frame}{Relaxação: Ignorando a Restrição}
    Ignorando por um momento a restrição $A = DE$:

    \begin{itemize}
        \item O problema vira mínimos quadrados clássico.
        \item Uma solução analítica está disponível.
    \end{itemize}

    A solução é:

    \[
    A^\star = (X X^\top)^{-1}(X X^\top).
    \]

    Se $(X X^\top)$ é inversível:

    \[
    A^\star = I,
    \]

    ou seja, o mapeamento ótimo (sem a restrição) seria a identidade.
\end{frame}


\begin{frame}{Usando a SVD de \(X X^\top\)}
    Como $X X^\top$ é quadrada, sua decomposição SVD é:

    \[
    X X^\top = U S U^\top,
    \]

    onde $U$ é ortogonal e $S$ contém os autovalores.

    Substituindo na solução:

    \[
    A^\star = (U S U^\top)^{-1}(U S U^\top)
    \]

    \[
    = (U^\top)^{-1} S^{-1} U^{-1} \; U S U^\top
    \]

    \[
    = U U^\top.
    \]
\end{frame}


\begin{frame}{Solução Analítica do Autoencoder Linear}
    Reintroduzindo a restrição $A = DE$ obtemos:

    \[
    A^\star = D^\star E^\star = U U^\top.
    \]

    Portanto:

    \[
    D^\star = U, \qquad E^\star = U^\top.
    \]

    \begin{itemize}
        \item Se $X$ tem posto $n$, somente as primeiras $n$ colunas de $U$ são necessárias.
        \item Assim, o autoencoder linear aprende o mesmo subespaço do PCA.
    \end{itemize}
\end{frame}


\begin{frame}{Interpretação: Relação Explícita com PCA}

    A matriz $U$ contém os \textbf{autovetores} dos dados —  
    exatamente as \textbf{direções principais} do PCA.

    \vspace{3mm}

    Portanto, a solução ótima do autoencoder linear é:

    \[
        E^\star = U^\top 
        \quad \text{(projeção para o espaço PCA)}
    \]

    \[
        D^\star = U
        \quad \text{(reconstrução a partir das componentes principais)}
    \]

    \vspace{3mm}

    \begin{itemize}
        \item  O \textbf{encoder} recupera as coordenadas PCA.
        \item  O \textbf{decoder} reconstrói os dados a partir dessas coordenadas.
    \end{itemize}

\end{frame}

\section{Considerações sobre Arquitetura}

\begin{frame}{Autoencoders neurais: Número de camadas}

    \textbf{Teorema do Aproximador Universal:} Redes neurais com uma única camada oculta podem aproximar qualquer função contínua.

    \vspace{4mm}

    \textbf{Autoencoders profundos:} Na prática, encoder e decoders possuem pelo menos uma camada oculta cada
    \begin{itemize}
        \item Maior capacidade de modelagem
        \item Podem aprender representações hierárquicas
        \item Podem ser treinados camada a camada (greedy layer-wise pretraining)
    \end{itemize}
\end{frame}

\begin{frame}{Dimensão do Espaço Latente}

  \textbf{Subcompletos} (undercomplete): Espaço latente tem dimensão menor que a entrada ($\dim(\mathcal{H}) < \dim(\mathcal{X})$)
  \begin{itemize}
    \item Forçam compactação dos dados (com perdas)
    \item Encoders e decoders não podem ser bons demais
  \end{itemize}
  \vspace{4mm}

  \textbf{Sobrecompletos} (overcomplete): Espaço latente pode ter dimensão maior que a entrada ($\dim(\mathcal{H}) > \dim(\mathcal{X})$)
  \begin{itemize}
    \item Risco de aprender a função identidade
    \item Usam \textbf{regularização} para conferir características desejáveis
  \end{itemize}


\end{frame}

\section{Autoencoders com regularização}
\begin{frame}{Autoencoders com regularização}

  Técnicas que adicionam \textbf{termos de regularização} ou modificam o processo de treinamento para melhorar a qualidade das representações aprendidas. São frequentemente \textbf{sobrecompletos}.

  \vspace{4mm}

  \begin{itemize}
    \item \textbf{Autoencoders Esparsos:} Força esparsidade na representação latente
    \item \textbf{Denoising Autoencoders:} Perturba a entrada com ruído e reconstrói a entrada limpa
    \item \textbf{Contractive Autoencoders:} Penaliza o jacobiano do encoder em relação à entrada
    \item \textbf{Variational Autoencoders:} Espaço latente se torna uma distribuição probabilística
  \end{itemize}

\end{frame}

\begin{frame}{Sparse Autoencoders}

    \textbf{Objetivo:} Forçar a representação latente a ser esparsa, ou seja, a maioria dos neurônios na camada latente deve estar inativa (valores próximos de zero).

    \vspace{2mm}

    \textbf{Função de Custo Modificada:}
    $$
    L(\theta, \phi) = \|x - \hat{x}\|^2_2 + \alpha \|h\|_1
    $$

\end{frame}

\section{Referências Bibliográficas}
\begin{frame}[allowframebreaks]
        \frametitle{Referências Bibliográficas}
        \bibliographystyle{ieeetr}
        \bibliography{presentation_bib.bib}
\end{frame}

\backmatter
\end{document}
